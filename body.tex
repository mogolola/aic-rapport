%% Copyright (C) 2008 Johan Oudinet <oudinet@lri.fr>
%%  
%% Permission is granted to make and distribute verbatim copies of
%% this manual provided the copyright notice and this permission notice
%% are preserved on all copies.
%%  
%% Permission is granted to process this file through TeX and print the
%% results, provided the printed document carries copying permission
%% notice identical to this one except for the removal of this paragraph
%% (this paragraph not being relevant to the printed manual).
%%  
%% Permission is granted to copy and distribute modified versions of this
%% manual under the conditions for verbatim copying, provided that the
%% entire resulting derived work is distributed under the terms of a 
%% permission notice identical to this one.
%%  
%% Permission is granted to copy and distribute translations of this manual
%% into another language, under the above conditions for modified versions,
%% except that this permission notice may be stated in a translation
%% approved by the Free Software Foundation
%%  


\chapter{Method}
\section{Data Description}
\subsection{Storm track data}
The raw storm track data used in this research is composed of more than 3000 extra-tropical and tropical storm tracks since 1979 extracted from the NOAA database IBTrACS\cite{knapp2010international}, see Figure \ref{fig:storm_tracks}. The tracks are defined by the 6-hourly center locations (latitude and longitude) for the entire lives of the storms. They come from both hemispheres and the number of records per storm varies from 2 to 120 time steps. In total, the database counts more than 90 000 time steps. 
\begin{figure}
	\begin{center}
		\epsfxsize=0.75\hsize \epsfbox{figs/all_storms.png}
	\end{center}
	\caption{Database: more than 3000 tropical/extra-tropical storm
		tracks since 1979. Dots = initial position, colors = maximal storm strength according to the Saffir-Simpson scale.}
	\label{fig:storm_tracks}
\end{figure}
\subsection{Reanalysis}
Reanalysis is a systematic approach to produce data sets for climate monitoring and research\cite{Reanalysis}. Reanalyses are created via an unchanging ("frozen") data assimilation scheme and model(s) which take all available observations as inputs every 6-12 hours over the period being analyzed. The input raw data can include but not limited to radiosonde, satellite, buoy, aircraft and ship reports. The framework generate dynamically consistent estimate of the climate atmospheric fields or state at each time point. Reanalyses can cover the entire globe from the Earthâ€™s surface to well above the stratosphere and have hundreds of available variables. The data is structured and easy to handle from a processing standpoint. Basically the model resolution and biases have been steadily improved over time. While Reanalyses also have limitation. The reliability of the data can vary depending on the location, time period, and variables considered due to observational constraints. 

The ERA-Interim is one of the reanalysis data sets that cover the data-rich period since 1979 to date. The ERA-Interim is the latest global atmospheric reanalysis produced by The European Centre for Medium-Range Weather Forecasts (ECMWF) and is continued in real time. The spectral resolution is T255 (about 80 km) and there are 60 vertical pressure levels, with the model top at 0.1 hPa (about 64 km). 

\section{Feature selection}
In the research we use storm track data and reanalysis to predict hurricane's track in the future. To capture the movement of a storm, we have classified 4 sources of information: 
\begin{enumerate}[leftmargin=2em]
	\item \textbf{The wind fields.}The trajectory of a storm depends on large scale atmospheric flows. Wind fields are the direct observation of the atmospheric flows. Thus, we extracted the wind fields of the neighborhood of the storm at every time step from the ERA-interim reanalysis database \cite{dee2011era}. Specifically, we extracted the u-wind and v-wind fields on a $25x25$ degree grid centered on the current storm location, at 3 atmospheric pressure levels (700 hPa, 500 hPa and 225 hPa).The choice of the 3 pressure levels was driven by statistical forecast models \cite{demaria2005further}. 
	\item \textbf{The geopotential height fields.} Atmospheric flows are directly due to the existence of \textbf{pressure gradients}, because particles in a fluid manner naturally flow from areas of higher pressure to areas of lower pressure. Geopotential height is a vertical coordinate referenced to Earth's mean sea level which is an adjustment of geometric height using a variation of gravity with latitude and evaluation (gravity changes with different latitude). Geopotential height has a positive correlation with pressure in a certain \textbf{pressure level}. For example, if somewhere has a higher geopotential height in a certain pressure level, it means that at the same geometric level that place has a higher pressure. In meteorology, scientists often use geopotential height as a function of pressure to facilitate calculation. Similar to wind fields, we extracted the geopotential height fields of the neighborhood of the storm at every time step on a $25x25$ degree grid centered on the current storm location, at 3 atmospheric pressure levels (700 hPa, 500 hPa and 225 hPa).
	
	\item \textbf{Displacement in history} A storm's future displacement can be predicted from his historical displacement in a statistical approach.
	
	\item \textbf{Other hand-crafted features.}
	Other useful features we extracted from a storm are: \textbf{current latitude and longitude}, \textbf{current windspeed} at the center of the storm, \textbf{Jday predictor}(Gaussian function of "Julian day of storm init - peak day of the hurricane season"\cite{demaria2005further}), and \textbf{current distance to land}. We call them meta data.
\end{enumerate}

 Another reason why we focused on the wind and geopotential parameters is that we applied a sparse feature selection technique (Automatic Relevance Determination, based on linear regression) over all available reanalysis fields, which highlighted the usefulness of wind.

\section{Overview of Proposed Method}

Because of the different nature of the wind field images, geopotential field images, and the past track data, it is not straightforward to mix them as a common input to a bigger network. We propose a fusion neural network architecture taking into account past trajectory data and reanalysis atmospheric physical fields images. An overview of the architecture is shown in Fig \ref{fig:fusion_arch}. We devise our fusion architecture accordingly, dividing into 3 streams: \textbf{Wind CNN}, \textbf{Pressure CNN} and \textbf{Past tracks + meta NN}. Wind CNN and Pressure CNN are convolutional neural networks that take atmospheric fields as input, Past tracks + meta NN is a small neural network which take 0D features as input. Each stream network is supposed make prediction independently. We fine-tune the parameters of each individual stream network for the same task of predicting the forecast track. Then we integrate the three networks into a fusion networks and retrain the parameters. The different steps will be outlined in the following sections.



\begin{figure} 
	\begin{center}
		\epsfxsize=0.95\hsize \epsfbox{figs/fusion_network.pdf}
	\end{center}
	\caption{General architecture: the three types of data are feeding three neural networks trained separately. The final fused network is re-trained before predicting the forecast track}
	\label{fig:fusion_arch}
\end{figure}

\section{Wind CNN and Pressure CNN}
In this section, we discuss the configuration of Wind CNN and Pressure CNN(the blue and yellow streams in Fig. \ref{fig:fusion_arch}).
The two networks are described in the same section because they are very similar in which they both extract information from atmospheric fields. The input data for Wind CNN and Pressure CNN is the centered atmospheric fields at different pressure levels at current time step as well as its consecutive time steps in history. An essential problem is how many time steps should be taken at each time. We started by taking wind fields measured at $t$ and $t - 6h$ at the same locations. We use a simple data structure: The input of Wind CNN can be seen as 12 images of size 25x25, including u, v components of wind fields at 3 atmospheric pressure levels (700 hPa, 500 hPa and 225 hPa) at the two time steps. And the input of Pressure CNN has 6 images of size 25x25 , including geopotential height fields at the three pressure levels at the two time steps.

\subsection{Architecture}
 
  We used as a guideline a typical CNN architecture alternating convolutional layers and max-pooling layers and added several fully connected layers at the end of the network \cite{simonyan2014very}. All hidden layers are equipped with the rectification (ReLU) non-linearity. Batch normalization is used after each Conv layer and FC layer except the last layer. To measure the difference in performance brought by increasing number of convolutional layers, we have evaluated several configurations of Wind CNN and Pressure CNN. 
  
  The configurations we have evaluated for Wind CNN and Pressure CNN are outlined in Table \ref{table:vgg_archs}, one per column. All configurations follow the generic design described above, and differ only in depth. In Table \ref{table:neuron_nums} we have shown the number of learnable parameters in each of those configurations. We note that those configurations have almost the same number of learnable parameter despite their different depths. Training details and results can be found in the next chapter where all experiments are described.
 
\begin{table}[]
	\begin{center}
		\caption{\textbf{ConvNet configurations}(shown in columns). The depth of the configurations increases from left(A) to the right(D), as more layers are added. The convolutional layer parametres are denoted as "conv(kernel size)-(number of output channels)". The ReLU activation layer and Batch normalization layers are not shown in figure }
		\begin{tabular}{|c|c|c|c|}
			\hline
			\multicolumn{4}{|c|}{ConvNet Configurations}                                                                                                                                                                                                                                                                                                                         \\ \hline
			A                                                                   & B                                                                              & C                                                                                          & D                                                                                                                \\ \hline
			7 layers                                                            & 8 layers                                                                       & 9 layers                                                                                   & 10 layers                                                                                                        \\ \hline
			\multicolumn{4}{|c|}{input (25*25, 12 channels image)}                                                                                                                                                                                                                                                                                                               \\ \hline
			\begin{tabular}[t]{@{}c@{}}conv3-32\\ maxpool\end{tabular} & \begin{tabular}[t]{@{}c@{}}conv3-32\\ conv3-32\\ maxpool\end{tabular} & \begin{tabular}[t]{@{}c@{}}conv3-64\\ conv3-64\\ maxpool\\ conv3-256\end{tabular} & \begin{tabular}[t]{@{}c@{}}conv3-64\\ conv3-64\\ maxpool\\ conv3-128\\ conv3-256\\ maxpool\end{tabular} \\ \hline
			\multicolumn{4}{|c|}{FC-576}                                                                                                                                                                                                                                                                                                                                         \\ \hline
			\multicolumn{4}{|c|}{FC-128}                                                                                                                                                                                                                                                                                                                                         \\ \hline
			\multicolumn{4}{|c|}{FC-64}                                                                                                                                                                                                                                                                                                                                          \\ \hline
			\multicolumn{4}{|c|}{FC-8}                                                                                                                                                                                                                                                                                                                                           \\ \hline
			\multicolumn{4}{|c|}{FC-8}                                                                                                                                                                                                                                                                                                                                           \\ \hline
			\multicolumn{4}{|c|}{FC-2}                                                                                                                                                                                                                                                                                                                                          \\ \hline
		\end{tabular}
	\end{center}
	\label{table:vgg_archs}
\end{table}

\begin{table}[]
	\centering
	\caption{Number of parameters (in millions)}
	\begin{tabular}{|l|c|c|c|c|}
		\hline
		Network              & A    & B    & C    & D    \\ \hline
		Number of parameters & 2.27 & 2.33 & 2.75 & 2.67 \\ \hline
	\end{tabular}
	\label{table:neuron_nums}
\end{table}



\section{Past tracks + meta NN}
Another important source of information is the previous displacements and the other hand crafted features that we mentioned in the previous part of this chapter. Temporal patterns can be extracted from a storm's historical tracks. Other hand crafted features can also provide additional (and important) clue for prediction. We designed a small neural network (two small fully connected layers, the green stream in Fig.\ref{fig:fusion_arch}) able to learn the future track from this past track and other hand crafted data. The past displacements of a storm are defined as the storm's displacement in every 6 hours in history. An important question is how long should we take from a storm's past tracks. We have evaluated the network for the task of predicting storm displacement in 24 hours using different length of past tracks. We finally decided to use 2 past tracks (the displacement from $t-12h$ to $t-6h$ and from $t-6h$ to $t$) because we noted that using longer past tracks doesn't improve the performance significantly. 
\begin{figure} 
	\begin{center}
		\epsfxsize=0.95\hsize \epsfbox{figs/fusion_details.pdf}
	\end{center}
	\caption{Connection of neurons in fusion layers before training fusion network. }
	\label{fig:fusion_details}
\end{figure}
\section{Fusion Network}
It is demonstrated that the three network: Wind CNN,  Pressure CNN,  Past tracks + Meta NN can independently predict a storm's future displacement. We then consider to improve the performance by integrating the three networks into a fusion network. Once the three individual stream networks are trained, we concatenate their certain number of last layers and add an additional layer at the end of the network as fused output layer. An example of fusion network is shown in Fig.\ref{fig:fusion_arch} where the last two layers of each stream networks are concatenated and merge into the fusion network. 

Figure \ref{fig:fusion_details} zooms in the fusion layers and explains how neurons are connected before training the fusion network. After fusing, the connections in previous stream networks(blue, yellow and green) are retained, no connection exist between different streams. The fusion output (red in Figure \ref{fig:fusion_details}) is the average of the three stream networks' outputs. 

We are also interested in different possible configurations of the fusion network. To be more specific, how many layers should be fused. We have carried out a through out evaluations on the performance of different configurations. Details and results will be shown in the next chapter. 








\section{Implementation Framework}
In the previous sections of the chapter we presented the details of the network configurations. In this section, we describe the algorithmic details of network training and evaluation in this research.

\subsection{Input preprocessing}
 Following the machine learning standard, we divided the entire dataset into train (60\%) / validation (20\%) / test (20\%). To avoid data leakage, we first separate all the storms into the three bins. Then, within each set, all time instants from the storms were treated  independently. It ensures that training examples, validation examples and test examples are extracted from different storms. Before being fed into the models, the input data is first standardized by subtracting mean value then dividing standard variation of each layer, computed on training set, from each pixel. 

\subsection{Training}  
Let $D = \{(x_w^1, x_p^1, x_{0d}^1, loc_t^1, loc_{t+\delta t}^1), ..., (x_w^n, x_p^n, x_{0d}^n, loc_t^n, loc_{t+\delta t}^n)\}$ be the labeled data for training our fusion network; with $x_w^i$, $x_p^i$, $x_{0d}^i$ denoting wind fields, pressure fields, 0d features respectively and $loc_t^i$, $loc_{t+\delta t}^i$ corresponding to the ground truth location at current time and after $\delta t$ time. $\delta t$ can be 6 hours, 12 hours, 18 hours, 24 hours... depending on the specific forecasting task. We train our model using a two-stage approach.


\begin{enumerate}[leftmargin=2em]
	\item \textbf{Training the stream networks: }\\
	We first proceed by training the individual stream networks. For training Wind CNN, Let $F_{wind}$ be the mapping function of Wind CNN. Learning $F_{wind}$ requires the estimation of the network parameters $\theta_w$ which is carried out by minimizing the loss between the forecast storm location and the corresponding ground truth location.  The forecast value and ground truth is in latitude and longitude. We first transform their differences in latitude and longitude into distance in kilometers. Let $F_{transform}$ be the transformation function from differences in latitude and longitude to distance. Then we use Mean Squared Error (MSE) in kilometers as the loss function:
	
	\begin{equation}
	\label{eq}
	L(\theta_w) = \frac{1}{n}\sum_{i=1}^{n} \left \| {F_{transform}((F_{wind}(x_w^i; \theta_w)} + loc_{i\ t} ) - loc_{i\ t+\delta t} ) \right \| ^2
	\end{equation}
	Where n is the total number of instances. Using MSE as the loss function can effectively penalize on large errors. Training the Pressure CNN and Past tracks + meta NN can be performed by an analogous way. After training, the three stream networks can be used to perform separate prediction of each modality. 
	\item \textbf{Training the fusion network: }\\
	The training of the fusion network follows two stages. We divide the layers of the fusion network into 2 parts: fusion layers (layers that are fused) and stream layers (layers that are retained from stream networks). The division is clearly shown in Fig.\ref{fig:fusion_arch}. In the first stage, we optimize only the weights in the fusion layers (keeping the weights of the stream layers intact). In the second stage, we let the weights in the whole network be equally optimized. Let $F_{f}$ be the function of fusion network, $\theta_{f}$, $\theta_{s}$ be parameters of fusion layers and of stream layers. Analogous to Eq.\ref{eq}, the first stage's loss function would be:
	
	\begin{equation}
	\label{eq_fusion_1}
	L(\theta_{f}) = \frac{1}{n}\sum_{i=1}^{n} \left \| {F_{transform}((F_{f}(x_w^i, x_p^i, x_{0d}^i; \theta_{f})} + loc_{i\ t} ) - loc_{i\ t+\delta t} ) \right \| ^2
	\end{equation}
	
	And second stage's loss function would be:
	\begin{equation}
	\label{eq_fusion_2}
	L(\theta_{f}, \theta_{s}) = \frac{1}{n}\sum_{i=1}^{n} \left \| {F_{transform}((F_{f}(x_w^i, x_p^i, x_{0d}^i; \theta_{f}, \theta_{s})} + loc_{i\ t} ) - loc_{i\ t+\delta t} ) \right \| ^2
	\end{equation}
	
	 We found that having the two stage of optimization augment the performance compared to optimize everything from the beginning. 

	 
	
	
	
\end{enumerate}



\subsection{Implementation Details}

The training was performed by mini-batch gradient descent with Adam optimizer. The batch size is set to 256. The training is regularized by weight decay (the $L_2$ penalty multiplier set to 0.01). Initial learning rates are fine-tuned individually in each model. The weights of the model were initialized followed by He initialization\cite{he2015delving}. Each model converges within 200 epochs, all sample are permuted after each epoch. Each evaluation is repeated by 3 times with different randomization and then compute an average score. Our implementation uses PyTorch 4.0. All training and evaluation are processed on 2 TitanX GPUs with data parallelism\cite{KRIZHEVSKY2014}. 


\chapter{Experiments and Result Analysis}
The chapter is organized into four sections. The evaluation of configurations for Wind CNN and Pressure CNN is shown in the first section. After choosing the proper ConvNet configuration for each stream network, in the second chapter we describe the process of selecting fusion network configuration. In the third section we evaluate our fusion network architecture with baseline and existing climate models. Finally the advantages are limitations are discussed in the last section.  

\section{Fusion Network Configuration}
Selecting the fusion network's configuration can be seen as a special hyper-parameter tunning. In the previous chapter we explained the method of confirming the fusion network configuration In this section we will describe the related experiments and results. Evaluations about selecting network configurations are processed on validation set since it is part of hyper-parameter tunning. The process can be identified into two steps. Our fusion approach aims at combining the stream networks. In the first step, the stream networks' configurations are determined. In the second step, we explore different fusion strategies and select the fusion network configuration with best performance. The two steps are described as follows:
\subsection{Selecting Stream Network Configuration}
In the previous chapter, we have outlined the four candidate architectures (shown in Table. \ref{table:arch_evaluation}) for Wind CNN and Pressure CNN. We evaluated their performance on 24 hours storm track prediction. The result of the evaluation on validation set (Wind CNN) is shown in Table \ref{table:arch_evaluation}. We give two scores: Mean Square Error(MSE) and Mean Absolute Error(MAE). Specifically the MAE denotes the model's mean prediction error in kilometers. With the increase of depth, we observed very unobvious improvement on the result. A possible reason could be that climate patterns are so simple that don't need more convolutional layers. Another possible reason is that it is underfitting due to lack of labeled training data. Nevertheless, more convolutional layers means that the network can learn features at more levels of abstraction. Finally we choose Network C after compromising between depth and computational efficiency.

Another experiment is to evaluate how adding more temporal part to the input data structure can improve the performance. We then added storm fields data at the same location at more consecutive time steps to input data. We process the input data on the same architecture and don't observe obvious improvement. Then we decided to use only information at $t$ and $t-6h$. 

\begin{table}[]
	\centering
	\caption{Performance of candidate configurations (Wind CNN) on 24 hours storm track prediction on validation set using wind fields}
	\label{table:arch_evaluation}
	\begin{tabular}{|c|c|c|}
		\hline
		Model(in Table. \ref{table:vgg_archs}) & Mean Sqaure Error (km\^2) & Mean Abosolute Error(km) \\ \hline
		A & 31430.08 & 145.43  \\ \hline
		B & 31761.95 & 146.62  \\ \hline
		C & 31552.91 & 145.5997 \\ \hline
		D & 31772.62 & 146.73 \\ \hline
		
	
	\end{tabular}
\end{table}



\subsection{Selecting Fusion Network Configuration}
After confirming stream networks' configuration, we then proceed to explore different fusion strategies.To figure out what fusion strategy has the best performance, two scenarios are considered. 1) how many layers should be fused? 2) Does fusing three streams outperforms fusing two streams or using single stream?

To answer the first question, we have evaluated 4 fusion networks that are based on the same three pre-trained stream networks and only differ in number of fused layers. The result is shown in Table \ref{table:fus_compare}. The fusion network that fuse 2 FC performs slightly better than others but generally the 4 networks have the same level performance. 

To answer the second question, we have compared the fusion network fusing all three streams with networks fusing two streams and single stream networks. The result is shown in Table \ref{table:fus_compare_}. We can clearly see the improvement of fusing all three stream networks with respect to fusing two stream networks and using single stream networks. 

Figure \ref{fig:boxplots} shows the 24h-forecast results on the test set in absolute distance error. It is verified that the fusion network fusing all three stream networks outperform other networks in Table \ref{table:fus_compare_}. 


\begin{table}[]
	\centering
	\caption{Comparison of fusion models performance on 24 hours storm track prediction on validation set}
	\label{table:fus_compare}
	\begin{tabular}{|c|c|c|}
		\hline
		Model & Mean Sqaure Error($km^2$) & Mean Abosolute Error($km$) \\ \hline
		Fus network, 3 streams, fuse 2 FC & 25453.48 & 130.04 \\ \hline
		Fus network, 3 streams, fuse 4 FC & 25628.22 & 130.27 \\ \hline
		Fus network, 3 streams, fuse 6 FC & 25846.74 & 130.95 \\ \hline
		Fus network, 3 streams, fuse 6 FC + 1 Conv layers & 25928.81 & 131.17 \\ \hline
	\end{tabular}
\end{table}


\begin{table}[]
	\centering
	\caption{Comparison of fusion models performance on 24 hours storm track prediction on validation set}
	\label{table:fus_compare_}
	\begin{tabular}{|c|c|c|}
		\hline
		Model & Mean Sqaure Error($km^2$) & Mean Abosolute Error($km$) \\ \hline
		0D NN & 51367.42 & 180.00 \\ \hline
		Pres. CNN, & 39387.76 & 164.91 \\ \hline
		Wind CNN & 30426.02 & 142.71 \\ \hline
		Fus network, 0D + Wind, fuse 2 FC  & 28723.56 & 136.64 \\ \hline
		Fus network, Pres. + Wind, fuse 2 FC & 28045.30 & 137.26 \\ \hline
		Fus network, Three streams, fuse 2 FC & 25453.48 & 130.04 \\ \hline
		
		
	\end{tabular}
\end{table}

\begin{figure}
	\begin{center}
		\epsfxsize=0.80\hsize \epsfbox{figs/arch_compare.png}
	\end{center}
	\caption{24h-forecast results on the test set (storms coming from all oceanic basins), in distance between predicted and real location. Baseline = previous displacement (going straight).}
	\label{fig:boxplots}
\end{figure}

\section{Comparison with existing forecasting models}
In this section we compare our fusion model CNN on 24 hours forecast with the existing forecasting models: OFCL and BCD5. BCD5 is a statistical model which is often used to benchmark other hurricane track forecasting methods. OFCL is the National Hurricane Center (NHC) official forecast. In general the performance of these models vary per year. With increased computational power, the performance of OFCL is constantly improving. In addition, we define the baseline prediction as equal to four times the last displacement (from $t-6h$ to $t$). 



\subsection{Quantitative}

\subsection{Qualitative}


%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "rapportM2R"
%%% End: 

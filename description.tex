%% Copyright (C) 2008 Johan Oudinet <oudinet@lri.fr>
%%  
%% Permission is granted to make and distribute verbatim copies of
%% this manual provided the copyright notice and this permission notice
%% are preserved on all copies.
%%  
%% Permission is granted to process this file through TeX and print the
%% results, provided the printed document carries copying permission
%% notice identical to this one except for the removal of this paragraph
%% (this paragraph not being relevant to the printed manual).
%%  
%% Permission is granted to copy and distribute modified versions of this
%% manual under the conditions for verbatim copying, provided that the
%% entire resulting derived work is distributed under the terms of a 
%% permission notice identical to this one.
%%  
%% Permission is granted to copy and distribute translations of this manual
%% into another language, under the above conditions for modified versions,
%% except that this permission notice may be stated in a translation
%% approved by the Free Software Foundation
%%  
\chapter{Background}
\label{sec:chapter2}

\section{Hurricane Trajectory Forecasting}
\label{sec:chapter2_1}
Cyclones, hurricanes or typhoons are words for the same phenomena: a rare and complex event characterized by strong winds surrounding a low-pressure area. Hurricanes are one of the most severe natural disasters that cost tremendous damage and death in each year. An average hurricane can release as much energy in a day as an explode of half a million small atomic bombs. Since 2000, over 45000 people were killed by hurricanes. In 2005, hurricane Katrina has cost as much as 125 billion US Dollars worth of damage. The explosion of the population also raises the risk of potential damage made by hurricanes.\cite{peduzzi2012global}. In 2010, It is estimated that 1.53 billion people live in hurricanes prone areas in 81 different countries and territories. 133.7 millions of people are exposed to hurricanes, the number has increased three times compared to the year 1970. That makes the forecast of hurricane trajectory so important, if we know when and where the hurricane is going to make landfall, people live in that area will be alarmed and get some time to protect their goods and evacuate. Improving the prediction of hurricane movements have enormous significance on society.

\subsection{Formation of Hurricanes}
The evolution and path of hurricanes depend on many factors at different scales and altitudes. All hurricanes are formed over warm ocean water near the equator. Warm and moist air over the ocean rises upward, which cause low-pressure area below. Air from surrounding areas with higher pressure flow over into low-pressure area and new air is heated and rise upward again. As warm and moist air rise and cools off, the water in the air forms clouds. The storm grows up and rotates as the cycle continue fed by ocean's heat and water evaporating from the surface. When the winds in the rotating storm reach 39mph, the storm is called a "tropical storm". When the wind speeds reach 74 mph, the storm is officially called a "tropical cyclone", or hurricane. The Saffirâ€“Simpson hurricane scale (SSHS), classifies hurricanes into 5 categories distinguished by the intensities of their sustained winds. The highest classification in the scale, category 5, consists of storms with sustained wind speeds higher than 156mph.\\
Hurricane can be very unpredictable. The movement of the hurricane sometimes performs loops, hairpin turns, and sharp curves. Its intensity can also change violently in a very short time. In 2005, After Hurricane Katrina made its first landfall in Florida it was only classified as a tropical storm. After emerged into the Gulf of Mexico, it rapidly strengthened into a Category 5 hurricane and its second landfall finally caused the largest damage from the natural disaster in history. Global warming is also considered to influence hurricane activities. it is estimated that global warming will cause hurricanes in the coming century to be more intense globally and have higher rainfall rates than present-day hurricanes \cite{knutson2013dynamical}, which may lead to both more representative and more consistent error statistics for forecasting.


\section{Forecast Method for Meteorologists}
Tracks and intensity are the two main goals of the forecast. Today, the forecasts (track and intensity) are provided by a numerous number of guidance models \cite{nhc_models}. The original best model were statistical models based on historical relationships between storm behavior and various other parameters. It was the major forecasting model until 1980's \cite{demaria2005further}. Today it is mostly used for testing and comparing new models. With the increase of computing capacity and data assimilation techniques, Dynamic model gradually replace statistical model. Dynamical models solve the physical equations governing motions in the atmosphere, they are very complex and computationally demanding. These models normally run on high-speed supercomputers. The bottleneck is the computing speed. If the computation lasts for days, it wouldn't make sense to make a prediction since the prediction will already be out of date. The statistical model, in contrast, does not demand high computational resources but are less accurate than dynamic models. Current national forecasts are typically driven by consensus methods able to combine different dynamical models.

Guidance models are characterized as either early or late, depending on whether or not they are available to the Hurricane Specialist during the forecast cycle. \cite{nhc_models} The late model can take hours to run. For example, if the NWS/Global Forecast System (GFS) runs at 12UTC, the result will not be available until 16UTC. The Dynamic models, in general, are late models. Although they can have a very precise forecast, they won't be able to release at once. But fortunately, there is a technique that allows to take the latest avaiable run of a late model and adjust its forecast to apply to the current synoptic time and initial conditions\cite{nhc_models}. Out of some reasons, 6h forecast is not on their official release.

\section{Convolutional Neural Network}
Recent advances in Deep Learning have shown promising results on enormous pattern recognition tasks, such as ImageNet Large Scale Visual Recognition Challenge (ILSVRC) \cite{russakovsky2015imagenet} \cite{krizhevsky2012imagenet}  \cite{szegedy2015going} and natural language processing \cite{goldberg2014word2vec} \cite{sutskever2014sequence}. Convolutional Neural Network (CNN) is one of the most important advance in Deep Learning. CNN is a type of deep, feed-forward artificial neural networks. An example of CNN is presented in Figure \ref{fig:LeNet} A typical CNN usually is comprised of several Conv layers followed by some fully connected layers. Between two successive Conv layers there is usually also sub-sampling layers also known as pooling layers. Connection between Conv layers is inspired by animal's cortex visual system, where each neuron only process data for its receptive field. Advantage of CNN is that it learns both spatial and temporal differences in different scales and thus could extract features automatically without learning huge amount of parameters. Now CNN is widely adopted as a very effective model for analyzing images or images-like data for pattern recognition.

The architecture of CNN has evolved over time in the last decade. Modern CNNs tend to be deep with large number of hidden layers. AlexNet is seen as a breakthrough that it won the 2012 ImageNet LSVRC-2012 competition by a large margin (reached 15.3\% compared to the former best 26.2\% in error rates on top-5 classification). AlexNet is comprised of 5 Conv layers followed by 3 Fully-connected layers. After that researchers continually introduce new architectures of CNNs which go deeper, more complex and have better precision on ImageNet. There are so many CNNs that work, one common argument is that they are basically empirical and tend to be hard to explain. The argument is still true now. Lots of works in CNNs show that CNNs have to have a deep network of layers in order for the hierarchical representation of visual data to work. VGG Net\cite{simonyan2014very} is one of the most influential publishments that give a guideline to design architecture of a CNN. A VGG Net applies alternatively Conv layers (use only 3x3 size kernel) and Maxpooling layers (use only 2x2 size kernel) through the whole network, and put several fully connected layers at the end of the network to generate output. The main argument is that rather than using relatively large receptive fields in the first Conv layers (e.g. 11x11 with stride 4 in AlexNet\cite{krizhevsky2012imagenet}), the author suggests to use very small 3x3 receptive fields throughout the whole network. The reason is that there could be more non-linear layers and smaller number of parameters, but effectively to have all scale of receptive fields. It turns out that the most important hyper-parameters tunning for VGG Net becomes tunning the depth of the network without worrying about other hyper-parameters like kernel size. It is the simplicity and effectiveness of VGG Net that makes it among the most popular CNN architectures. 

CNN is known tedious to train. To build an excellent CNN, researchers have proposed lots of tricks. Some of the tricks are very crucial to successfully train a CNN. Training a CNN with huge number of learnable parameters usually require of large number of examples. Data Augmentation methods such as flipping, randomly cropping could effectively increase the network's performance when there are only limited number of examples. Proper input data normalization and weights initialization are important steps to accelerate convergence during the training. To realize it, the input data is usually zero-centered and normalized at each channel to have a similar distribution. Weights are usually initialized by random weights drawn by Gaussian distributions\cite{krizhevsky2012imagenet}, but it cause difficulties of converging too slow when training very deep CNN. A paper shows that the reason is because of wrong norm. The author propose a method called 'Xavier' initialization\cite{glorot2010understanding} to ensure the outputs in each layer are initially standard normal distributed. 'Xavier' initialization is based on an assumption that activations are linear, which is not valid on rectifier nonlinearities(ReLU). Another paper suggest a more robust initialization that address for the rectifier nonlinearities\cite{he2015delving}. Batch Normalization(BN) layers are also recommended to be used to maintain the distribution of output layers during training, thus to accelerate convergence\cite{krizhevsky2012imagenet}. Other techniques like Regularizations, Optimization Algorithms, Ensemble, should also be taken account during the training.

\begin{figure}
	\begin{center}
		\epsfxsize=0.75\hsize \epsfbox{figs/tLKYz.png}
	\end{center}
	\caption{Architecture of LeNet, shows a series of layers in CNN that learn hierarchical features representation \cite{lecun1998gradient} }
	\label{fig:LeNet}
\end{figure}


\section{CNN in climate science}
The hurricane statistical forecasting models perform poorly with respect to dynamical models, even if the database made of past hurricane is constantly growing. The machine learning methods, able for example to capture non-linearity and complex relations, have only been scarcely tested. However, they have recently shown their efficacy in a various number of forecasting tasks. Particularly, convolutional neural networks (CNN) have raised attention as they are suited for large imaging data. A convolutional LSTM model has been used for precipitation forecast in a promising study \cite{xingjian2015convolutional}. Another recent study predicts the evolution of sea surface temperature maps by combining CNN with physical knowledge \cite{de2017deep}. The CNN has also been used for the detection of extreme weather like hurricanes from meteorological variables patches. \cite{racah2017extremeweather}. These studies show the strong potential on various climate problems. 

To our knowledge, only two preliminary studies have tackled the hurricane forecast tracking using machine learning: the first one uses random forests on local reanalysis histograms \cite{liberge2011prevision}, however it was tested only on tropical storms in 2015 in North Atlantic, and the mean error of 6h-forecasts seem to indicate poor results (more than 60km). The second uses a sparse recurrent neural network from trajectory data \cite{moradi2016sparse}, it was tested on only 4 hurricanes and seem also to give large distance errors (mean 6h-forecast error is 72km). We have noticed that Deep Learning as a generative method has not been tested before. In this study, we expect to explore Deep Convolutional Neural Network to be a new Hurricane Forecasting method, and hopefully could improve Hurricane Trajectory Forecasting results.

\section{Fusion}


%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "rapportM2R"
%%% End: 
